{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Results and Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pcg model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81/81 [00:00<00:00, 1746.41it/s]\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from classifier.datasets import HeartAudioDataset\n",
    "from classifier.model_factory import ModelFactory\n",
    "import torch\n",
    "\n",
    "model_path = \"../../models/wav2vec-4s-pcg-training-a-pcg-oct22.log-9\"\n",
    "model = \"wav2vec\"\n",
    "aux_type = model\n",
    "\n",
    "\n",
    "def load_all_pcg_ta_models(model_base_path, model_str, class_names):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    optimizer = 'sgd'\n",
    "    models_factory = ModelFactory(device, class_names, freeze=False, optimizer_type=optimizer)\n",
    "    models = []\n",
    "\n",
    "    for i in range(1,11):\n",
    "        model_name = f\"{model_base_path}-{i}\"\n",
    "        models.append(models_factory.load_model(model_name, model_str == \"inception\", model_str == \"wav2vec\"))\n",
    "\n",
    "    return models\n",
    "\n",
    "def load_pcg_ta_model(model_path, model_str, class_names):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    optimizer = 'sgd'\n",
    "    models_factory = ModelFactory(device, class_names, freeze=False, optimizer_type=optimizer)\n",
    "    model = models_factory.load_model(model_path, model_str == \"inception\", model_str == \"wav2vec\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_pcg_ta_dataset(fs=4125):\n",
    "    data_dir = \"../../data/physionet.org/files/challenge-2016/1.0.0/training-a\"\n",
    "    split_path = \"../../data/heart-sounds/actually-is-reference-CTH/REFERENCE.csv\"\n",
    "    audio_dir = \"../../data/preprocessed_audio/training-a-4s-for-wav2vec-paper\"\n",
    "    segment_dir = \"../../data/segmentation/wav2vec-training-a-4\"\n",
    "    database = \"training-a-pcg\"\n",
    "    segmentation = \"time\"\n",
    "    four_bands = False\n",
    "    skip_data_valid = False \n",
    "    sig_len = 4\n",
    "\n",
    "    Dataset = HeartAudioDataset\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    phases = ['test']\n",
    "    datasets = {p: Dataset(\n",
    "        data_dir,\n",
    "        split_path,\n",
    "        segment_dir,\n",
    "        p,\n",
    "        audio_dir,\n",
    "        ecg=(database == \"training-a\"),\n",
    "        segmentation=segmentation,\n",
    "        augmentation=False,\n",
    "        four_band=four_bands,\n",
    "        fs=fs,\n",
    "        skip_data_valid=skip_data_valid,\n",
    "        sig_len=sig_len,\n",
    "    ) for p in phases}\n",
    "\n",
    "    class_names = next(iter(datasets.values())).classes\n",
    "    optimizer = 'sgd'\n",
    "\n",
    "    # Setup dataset\n",
    "    for key in datasets.keys():\n",
    "        datasets[key].channel = 0\n",
    "\n",
    "    return datasets\n",
    "\n",
    "datasets = get_pcg_ta_dataset()\n",
    "class_names = next(iter(datasets.values())).classes\n",
    "\n",
    "model = load_pcg_ta_model(model_path, model, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ecg model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81/81 [00:00<00:00, 154.09it/s]\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from classifier.datasets import HeartAudioDataset\n",
    "from classifier.model_factory import ModelFactory\n",
    "import torch\n",
    "\n",
    "model_path = \"../../models/wav2vec-4s-ecg-training-a-ecg-oct23.log-2\"\n",
    "model = \"wav2vec\"\n",
    "aux_type = model\n",
    "\n",
    "def load_all_ecg_ta_models(model_base_path, model_str, class_names):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    optimizer = 'sgd'\n",
    "    models_factory = ModelFactory(device, class_names, freeze=False, optimizer_type=optimizer)\n",
    "    models = []\n",
    "\n",
    "    for i in range(1,11):\n",
    "        model_name = f\"{model_base_path}-{i}\"\n",
    "        models.append(models_factory.load_model(model_name, model_str == \"inception\", model_str == \"wav2vec\"))\n",
    "\n",
    "    return models\n",
    "\n",
    "def load_ecg_ta_model(model_path, model_str, class_names):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    optimizer = 'sgd'\n",
    "    models_factory = ModelFactory(device, class_names, freeze=False, optimizer_type=optimizer)\n",
    "    model = models_factory.load_model(model_path, model_str == \"inception\", model_str == \"wav2vec\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_ecg_ta_dataset(fs=4125):\n",
    "    data_dir = \"../../data/physionet.org/files/challenge-2016/1.0.0/training-a\"\n",
    "    split_path = \"../../data/heart-sounds/actually-is-reference-CTH/REFERENCE.csv\"\n",
    "    audio_dir = \"../../data/preprocessed_audio/training-a-4s-for-wav2vec-paper\"\n",
    "    segment_dir = \"../../data/segmentation/wav2vec-training-a-4\"\n",
    "    database = \"training-a-ecg\"\n",
    "    segmentation = \"time\"\n",
    "    four_bands = False\n",
    "    skip_data_valid = False \n",
    "    sig_len = 4\n",
    "\n",
    "    Dataset = HeartAudioDataset\n",
    "    phases = ['test']\n",
    "    datasets = {p: Dataset(\n",
    "        data_dir,\n",
    "        split_path,\n",
    "        segment_dir,\n",
    "        p,\n",
    "        audio_dir,\n",
    "        ecg=(database == \"training-a\"),\n",
    "        segmentation=segmentation,\n",
    "        augmentation=False,\n",
    "        four_band=four_bands,\n",
    "        fs=fs,\n",
    "        skip_data_valid=skip_data_valid,\n",
    "        sig_len=sig_len,\n",
    "    ) for p in phases}\n",
    "\n",
    "    class_names = next(iter(datasets.values())).classes\n",
    "    optimizer = 'sgd'\n",
    "\n",
    "    # Setup dataset\n",
    "    for key in datasets.keys():\n",
    "        datasets[key].channel = 1\n",
    "\n",
    "    return datasets\n",
    "\n",
    "datasets = get_ecg_ta_dataset()\n",
    "class_names = next(iter(datasets.values())).classes\n",
    "#\n",
    "model = load_ecg_ta_model(model_path, model, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load PECG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81/81 [00:00<00:00, 1233.42it/s]\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from classifier.datasets import HeartAudioDataset\n",
    "from classifier.model_factory import ModelFactory\n",
    "import torch\n",
    "\n",
    "model = \"wav2vec\"\n",
    "aux_type = model\n",
    "model_path = \"../../models/wav2vec-4s-training-a-split-1-jan17.log.pt-1\"\n",
    "\n",
    "def load_all_pecg_ta_models(model_base_path, model_str, class_names):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    optimizer = 'sgd'\n",
    "    models_factory = ModelFactory(device, class_names, freeze=False, optimizer_type=optimizer)\n",
    "    models = []\n",
    "\n",
    "    for i in range(1,11):\n",
    "        model_name = f\"{model_base_path}-{i}\"\n",
    "        models.append(models_factory.load_model(model_name, is_large_wav2vec=True))\n",
    "\n",
    "    return models\n",
    "\n",
    "def load_pecg_ta_model(model_path, model_str, class_names):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    optimizer = 'sgd'\n",
    "    models_factory = ModelFactory(device, class_names, freeze=False, optimizer_type=optimizer)\n",
    "    model = models_factory.load_model(model_path, is_large_wav2vec=True)\n",
    "    model.model_ft.to(device)\n",
    "    for submodel in model.model_ft.models:\n",
    "        submodel.to(device)\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_pecg_ta_dataset(fs=16000, augment=False):\n",
    "    data_dir = \"../../data/physionet.org/files/challenge-2016/1.0.0/training-a\"\n",
    "    split_path = \"../../data/heart-sounds/actually-is-reference-CTH/REFERENCE.csv\"\n",
    "    audio_dir = \"../../data/preprocessed_audio/training-a-4s-for-wav2vec-paper\"\n",
    "    segment_dir = \"../../data/segmentation/wav2vec-training-a-4\"\n",
    "    database = \"training-a\"\n",
    "    segmentation = \"time\"\n",
    "    four_bands = False\n",
    "    skip_data_valid = False\n",
    "    sig_len = 4\n",
    "\n",
    "    Dataset = HeartAudioDataset\n",
    "    phases = ['test']\n",
    "    datasets = {p: Dataset(\n",
    "        data_dir,\n",
    "        split_path,\n",
    "        segment_dir,\n",
    "        p,\n",
    "        audio_dir,\n",
    "        ecg=(database == \"training-a\"),\n",
    "        segmentation=segmentation,\n",
    "        augmentation=False,\n",
    "        four_band=four_bands,\n",
    "        fs=fs,\n",
    "        skip_data_valid=skip_data_valid,\n",
    "        sig_len=sig_len,\n",
    "    ) for p in phases}\n",
    "\n",
    "    class_names = next(iter(datasets.values())).classes\n",
    "    optimizer = 'sgd'\n",
    "\n",
    "    # Setup dataset\n",
    "    for key in datasets.keys():\n",
    "        datasets[key].channel = -1\n",
    "\n",
    "    return datasets\n",
    "\n",
    "datasets = get_pecg_ta_dataset()\n",
    "class_names = next(iter(datasets.values())).classes\n",
    "\n",
    "model = load_pecg_ta_model(model_path, model, class_names)\n",
    "model.set_mode('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81/81 [00:00<00:00, 967.67it/s]\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16500\n"
     ]
    }
   ],
   "source": [
    "from classifier.datasets import HeartAudioDataset\n",
    "from classifier.model_factory import ModelFactory\n",
    "import torch\n",
    "\n",
    "model_str = \"wav2vec-cnn\"\n",
    "aux_type = model_str\n",
    "model_path = \"../../models/wav2vec-cnn-4s-training-a-wav2vec_cnn-split-1-jan21.log.pt-1\"\n",
    "\n",
    "def load_all_pecg_ta_models(model_base_path, model_str, class_names):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    optimizer = 'sgd'\n",
    "    models_factory = ModelFactory(device, class_names, freeze=False, optimizer_type=optimizer)\n",
    "    models = []\n",
    "\n",
    "    for i in range(1,11):\n",
    "        model_name = f\"{model_base_path}-{i}\"\n",
    "        models.append(models_factory.load_model(model_name, is_large_wav2veccnn=True))\n",
    "\n",
    "    return models\n",
    "\n",
    "def load_pecg_ta_model(model_path, model_str, class_names):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    optimizer = 'sgd'\n",
    "    models_factory = ModelFactory(device, class_names, freeze=False, optimizer_type=optimizer)\n",
    "    model = models_factory.load_model(model_path, is_large_wav2veccnn=True)\n",
    "    model.model_ft.to(device)\n",
    "    for submodel in model.model_ft.models:\n",
    "        submodel.to(device)\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_pecg_ta_dataset(fs=16000, augment=False):\n",
    "    data_dir = \"../../data/physionet.org/files/challenge-2016/1.0.0/training-a\"\n",
    "    split_path = \"../../data/heart-sounds/actually-is-reference-CTH/REFERENCE.csv\"\n",
    "    audio_dir = \"../../data/preprocessed_audio/training-a-4s-for-wav2vec-paper\"\n",
    "    segment_dir = \"../../data/segmentation/wav2vec-training-a-4\"\n",
    "    database = \"training-a\"\n",
    "    segmentation = \"time\"\n",
    "    four_bands = False\n",
    "    skip_data_valid = False\n",
    "    sig_len = 4\n",
    "\n",
    "    Dataset = HeartAudioDataset\n",
    "    phases = ['test']\n",
    "    datasets = {p: Dataset(\n",
    "        data_dir,\n",
    "        split_path,\n",
    "        segment_dir,\n",
    "        p,\n",
    "        audio_dir,\n",
    "        ecg=(database == \"training-a\"),\n",
    "        segmentation=segmentation,\n",
    "        augmentation=False,\n",
    "        four_band=four_bands,\n",
    "        fs=fs,\n",
    "        skip_data_valid=skip_data_valid,\n",
    "        sig_len=sig_len,\n",
    "    ) for p in phases}\n",
    "\n",
    "    class_names = next(iter(datasets.values())).classes\n",
    "    optimizer = 'sgd'\n",
    "\n",
    "    # Setup dataset\n",
    "    for key in datasets.keys():\n",
    "        datasets[key].channel = -1\n",
    "\n",
    "    return datasets\n",
    "\n",
    "datasets = get_pecg_ta_dataset(fs=4125)\n",
    "class_names = next(iter(datasets.values())).classes\n",
    "\n",
    "model = load_pecg_ta_model(model_path, model_str, class_names)\n",
    "model.set_mode('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load PCG CinC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16500\n"
     ]
    }
   ],
   "source": [
    "# PCG CNN model\n",
    "from classifier.datasets import HeartAudioDatabase\n",
    "from classifier.model_factory import ModelFactory\n",
    "import torch\n",
    "\n",
    "model_path = '../../data/models/cinc-wav2vecwav2vec-4s-cinc-wav2vec-cnn-cinc-wav2vec-cnn-dec19.log-10.pth'\n",
    "#model_path = '../../data/models/cinc-wav2vecwav2vec-4s-cinc-cinc-jan06.log-10.pth'\n",
    "model = \"wav2vec-cnn\"\n",
    "aux_type = model\n",
    "\n",
    "def load_all_pcg_cinc_cnn_models(model_base_path, model_str, class_names):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    optimizer = 'sgd'\n",
    "    models_factory = ModelFactory(device, class_names, freeze=False, optimizer_type=optimizer)\n",
    "    models = []\n",
    "\n",
    "    for i in range(1,11):\n",
    "        model_name = f\"{model_base_path}-{i}.pth\"\n",
    "        models.append(models_factory.load_model(model_name, is_wav2veccnn=True))\n",
    "\n",
    "    return models\n",
    "\n",
    "def load_pcg_cinc_cnn_model(model_path, model_str, class_names):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    optimizer = 'sgd'\n",
    "    models_factory = ModelFactory(device, class_names, freeze=False, optimizer_type=optimizer)\n",
    "    model = models_factory.load_model(model_path, is_wav2veccnn=True)\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_pcg_cinc_dataset(fs=16000):\n",
    "    data_dir = \"../../data/physionet.org/files/challenge-2016/1.0.0/\"\n",
    "    split_path = \"../../data/splits/rnn\"\n",
    "    audio_dir = \"../../data/processed_audio/cinc/entire\"\n",
    "    segment_dir = \"../../data/segmentation/rnn\"\n",
    "    database = \"cinc\"\n",
    "    segmentation = \"time\"\n",
    "    four_bands = False\n",
    "    skip_data_valid = True \n",
    "    sig_len = 4\n",
    "\n",
    "    Dataset = HeartAudioDatabase\n",
    "    phases = ['test']\n",
    "    datasets = {p: Dataset(\n",
    "        data_dir,\n",
    "        split_path,\n",
    "        segment_dir,\n",
    "        p,\n",
    "        audio_dir,\n",
    "        ecg=(database == \"training-a\"),\n",
    "        segmentation=segmentation,\n",
    "        augmentation=False,\n",
    "        four_band=four_bands,\n",
    "        fs=fs,\n",
    "        skip_data_valid=skip_data_valid,\n",
    "        sig_len=sig_len,\n",
    "    ) for p in phases}\n",
    "\n",
    "    return datasets\n",
    "\n",
    "datasets = get_pcg_cinc_dataset(fs=4125)\n",
    "class_names = next(iter(datasets.values())).classes\n",
    "\n",
    "model = load_pcg_cinc_cnn_model(model_path, model, class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from classifier.datasets import HeartAudioDatabase\n",
    "from classifier.model_factory import ModelFactory\n",
    "import torch\n",
    "\n",
    "model_path = '../../models/models_for_maybe_paper/wav2vec-4s-cinc-16k-cinc-16k-dec16.log-4.pth'\n",
    "#model_path = '../../data/models/cinc-wav2vecwav2vec-4s-cinc-cinc-jan06.log-10.pth'\n",
    "model = \"wav2vec\"\n",
    "aux_type = model\n",
    "\n",
    "def load_all_pcg_cinc_models(model_base_path, model_str, class_names):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    optimizer = 'sgd'\n",
    "    models_factory = ModelFactory(device, class_names, freeze=False, optimizer_type=optimizer)\n",
    "    models = []\n",
    "\n",
    "    for i in range(1,11):\n",
    "        model_name = f\"{model_base_path}-{i}.pth\"\n",
    "        models.append(models_factory.load_model(model_name, is_wav2vec=True))\n",
    "\n",
    "    return models\n",
    "\n",
    "def load_pcg_cinc_model(model_path, model_str, class_names):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    optimizer = 'sgd'\n",
    "    models_factory = ModelFactory(device, class_names, freeze=False, optimizer_type=optimizer)\n",
    "    model = models_factory.load_model(model_path, is_wav2vec=True)\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_pcg_cinc_dataset(fs=16000):\n",
    "    data_dir = \"../../data/physionet.org/files/challenge-2016/1.0.0/\"\n",
    "    split_path = \"../../data/splits/rnn\"\n",
    "    audio_dir = \"../../data/processed_audio/cinc/entire\"\n",
    "    segment_dir = \"../../data/segmentation/rnn\"\n",
    "    database = \"cinc\"\n",
    "    segmentation = \"time\"\n",
    "    four_bands = False\n",
    "    skip_data_valid = True \n",
    "    sig_len = 4\n",
    "\n",
    "    Dataset = HeartAudioDatabase\n",
    "    phases = ['test']\n",
    "    datasets = {p: Dataset(\n",
    "        data_dir,\n",
    "        split_path,\n",
    "        segment_dir,\n",
    "        p,\n",
    "        audio_dir,\n",
    "        ecg=(database == \"training-a\"),\n",
    "        segmentation=segmentation,\n",
    "        augmentation=False,\n",
    "        four_band=four_bands,\n",
    "        fs=fs,\n",
    "        skip_data_valid=skip_data_valid,\n",
    "        sig_len=sig_len,\n",
    "    ) for p in phases}\n",
    "\n",
    "    return datasets\n",
    "\n",
    "datasets = get_pcg_cinc_dataset()\n",
    "class_names = next(iter(datasets.values())).classes\n",
    "\n",
    "model = load_pcg_cinc_model(model_path, model, class_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fragment Stats:\n",
      "test\n",
      "loss: 0.635, tpr: 0.831, tnr: 0.750, fpr: 0.250, ppv: 0.890, npv: 0.646, acc: 0.807, acc_mu: 0.790, qi: 0.789, j: 0.581, f1p: 0.859, f1n: 0.694, mcc: 0.558\n",
      "defaultdict(<class 'int'>, {'tn': 117, 'fp': 39, 'fn': 64, 'tp': 314})\n",
      "\n",
      "Patient Stats:\n",
      "test\n",
      "loss: 0.000, tpr: 0.860, tnr: 0.750, fpr: 0.250, ppv: 0.891, npv: 0.692, acc: 0.827, acc_mu: 0.805, qi: 0.803, j: 0.610, f1p: 0.875, f1n: 0.720, mcc: 0.596\n",
      "defaultdict(<class 'int'>, {'tn': 18, 'fp': 6, 'fn': 8, 'tp': 49})\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from classifier.dataloaders import create_dataloaders\n",
    "from classifier.testing import FineTunerFragmentTester, FineTunerPatientTester\n",
    "import torch\n",
    "\n",
    "dataloader = create_dataloaders(datasets, aux_type)\n",
    "\n",
    "fragment_tester = FineTunerFragmentTester(model, dataloader)\n",
    "patient_tester = FineTunerPatientTester(model, dataloader)\n",
    "\n",
    "print()\n",
    "fragment_tester.test()\n",
    "print()\n",
    "patient_tester.test()\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_all_ecg_ta_models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mclassifier\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtesting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FineTunerFragmentTester, FineTunerPatientTester\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m models \u001b[38;5;241m=\u001b[39m \u001b[43mload_all_ecg_ta_models\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../models/wav2vec-4s-ecg-training-a-ecg-oct23.log\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwav2vec\u001b[39m\u001b[38;5;124m\"\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models:\n\u001b[1;32m      8\u001b[0m     fragment_tester \u001b[38;5;241m=\u001b[39m FineTunerFragmentTester(model, dataloader)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_all_ecg_ta_models' is not defined"
     ]
    }
   ],
   "source": [
    "from classifier.dataloaders import create_dataloaders\n",
    "from classifier.testing import FineTunerFragmentTester, FineTunerPatientTester\n",
    "import torch\n",
    "\n",
    "models = load_all_ecg_ta_models(\"../../models/wav2vec-4s-ecg-training-a-ecg-oct23.log\", \"wav2vec\", [\"0\", \"1\"])\n",
    "\n",
    "for model in models:\n",
    "    fragment_tester = FineTunerFragmentTester(model, dataloader)\n",
    "    patient_tester = FineTunerPatientTester(model, dataloader)\n",
    "\n",
    "    print()\n",
    "    fragment_tester.test()\n",
    "    print()\n",
    "    patient_tester.test()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create ROC Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "from classifier.dataloaders import create_dataloaders\n",
    "from classifier.testing import FineTunerFragmentTester, FineTunerPatientTester\n",
    "\n",
    "def generate_roc_plots(models, datasets, output_path = None):\n",
    "    dataloader = create_dataloaders(datasets, aux_type)\n",
    "\n",
    "    fragment_tprs = []\n",
    "    fragment_fprs = []\n",
    "    patient_tprs = []\n",
    "    patient_fprs = []\n",
    "\n",
    "    for model in models:\n",
    "        model.model_ft = model.model_ft.to(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        fragment_tester = FineTunerFragmentTester(model, dataloader)\n",
    "        patient_tester = FineTunerPatientTester(model, dataloader)\n",
    "\n",
    "        fragment_tpr, fragment_fpr, _ = fragment_tester.roc_curve()\n",
    "        patient_tpr, patient_fpr, _ = patient_tester.roc_curve()\n",
    "\n",
    "        fragment_fprs.append(fragment_fpr)\n",
    "        fragment_tprs.append(fragment_tpr)\n",
    "        patient_fprs.append(patient_fpr)\n",
    "        patient_tprs.append(patient_tpr)\n",
    "        model.model_ft = model.model_ft.to(\"cpu\")\n",
    "\n",
    "\n",
    "    common_tpr = np.linspace(0, 1, 100)\n",
    "    interpolated_fragment_fprs = []\n",
    "    interpolated_patient_fprs = []\n",
    "\n",
    "    for fpr, tpr in zip(fragment_tprs, fragment_fprs):\n",
    "        interp_fpr = np.interp(common_tpr, fpr[::-1], tpr[::-1])\n",
    "        interpolated_fragment_fprs.append(interp_fpr)\n",
    "    for fpr, tpr in zip(patient_tprs, patient_fprs):\n",
    "        interp_fpr = np.interp(common_tpr, fpr[::-1], tpr[::-1])\n",
    "        interpolated_patient_fprs.append(interp_fpr)\n",
    "\n",
    "    interpolated_fragment_fprs = np.array(interpolated_fragment_fprs)\n",
    "    interpolated_patient_fprs = np.array(interpolated_patient_fprs)\n",
    "\n",
    "    mean_fpr = np.mean(interpolated_fragment_fprs, axis=0)\n",
    "    fpr_2_5 = np.percentile(interpolated_fragment_fprs, 2.5, axis=0)\n",
    "    fpr_97_5 = np.percentile(interpolated_fragment_fprs, 97.5, axis=0)\n",
    "    print(f\"{mean_fpr=}, {fpr_2_5=}, {fpr_97_5=}\")\n",
    "    plot_roc_curves([common_tpr, common_tpr, common_tpr], [mean_fpr, fpr_2_5, fpr_97_5], output_path=f\"{output_path}_fragment\")\n",
    "\n",
    "    mean_fpr = np.mean(interpolated_patient_fprs, axis=0)\n",
    "    fpr_2_5 = np.percentile(interpolated_patient_fprs, 2.5, axis=0)\n",
    "    fpr_97_5 = np.percentile(interpolated_patient_fprs, 97.5, axis=0)\n",
    "    print(f\"{mean_fpr=}, {fpr_2_5=}, {fpr_97_5=}\")\n",
    "    plot_roc_curves([common_tpr, common_tpr, common_tpr], [mean_fpr, fpr_2_5, fpr_97_5], output_path=f\"{output_path}_patient\")\n",
    "\n",
    "\n",
    "\n",
    "def plot_roc_curves(\n",
    "        tprs: list[list[float]], \n",
    "        tnrs: list[list[float]], \n",
    "        output_path: Optional[str] = None,\n",
    "        title: Optional[str] = None\n",
    "):\n",
    "    assert len(tprs) == len(tnrs), \"Should contain the same length arrays.\"\n",
    "\n",
    "    plt.figure()\n",
    "    plt.grid()\n",
    "    plt.xlabel(\"True positive rate\")\n",
    "    plt.ylabel(\"False positive rate\")\n",
    "    plt.title(\"ROC Curve\")\n",
    "\n",
    "    tpr = tprs[0]\n",
    "    tnr = tnrs[0]\n",
    "    plt.plot(tnr, tpr, label=\"Mean\")\n",
    "    \n",
    "    tpr = tprs[1]\n",
    "    tnr = tnrs[1]\n",
    "    plt.plot(tnr, tpr, label=\"2.5% CI\")\n",
    "\n",
    "    tpr = tprs[2]\n",
    "    tnr = tnrs[2]\n",
    "    plt.plot(tnr, tpr, label=\"97.5% CI\")\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    if output_path is None:\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('saving')\n",
    "        plt.savefig(output_path)\n",
    "\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib\n",
    "#matplotlib.use('TkAgg')  # or 'QtAgg' or 'MacOSX' depending on your system\n",
    "plt.ion\n",
    "\n",
    "models_base_path_4125 = '../../data/models/cinc-wav2vecwav2vec-4s-cinc-cinc-jan06.log'\n",
    "models_base_path_16 = '../../models/models_for_maybe_paper/wav2vec-4s-cinc-16k-cinc-16k-dec16.log'\n",
    "\n",
    "datasets_4125 = get_pcg_cinc_dataset(fs=4125)\n",
    "class_names = next(iter(datasets.values())).classes\n",
    "\n",
    "models_4125 = load_all_pcg_cinc_models(models_base_path_4125, \"wav2vec\", class_names)\n",
    "\n",
    "datasets_16 = get_pcg_cinc_dataset(fs=16000)\n",
    "class_names = next(iter(datasets.values())).classes\n",
    "\n",
    "models_16 = load_all_pcg_cinc_models(models_base_path_16, \"wav2vec\", class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_roc_plots(models_4125, datasets_4125, \"roc/cinc-4125\")\n",
    "generate_roc_plots(models_16, datasets_16, \"roc/cinc-16k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81/81 [00:00<00:00, 1099.18it/s]\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib\n",
    "#matplotlib.use('TkAgg')  # or 'QtAgg' or 'MacOSX' depending on your system\n",
    "plt.ion\n",
    "\n",
    "models_base_path_4125 = '../../models/wav2vec-4s-training-a-split-1-jan17.log.pt'\n",
    "\n",
    "models_base_path_16 = '../../models/models_for_maybe_paper/wav2vec-4s-training-a-16k-oct23.log'\n",
    "models_base_path_pcg = '../../models/wav2vec-4s-pcg-training-a-pcg-oct22.log'\n",
    "models_base_path_ecg = '../../models/wav2vec-4s-ecg-training-a-ecg-oct23.log'\n",
    "models_base_path_no_augment = '../../models/models_for_maybe_paper/wav2vec-4s-training-a-16k-no-augment-jan03.log'\n",
    "\n",
    "datasets_4125 = get_pecg_ta_dataset(fs=4125)\n",
    "class_names = next(iter(datasets_4125.values())).classes\n",
    "\n",
    "models_4125 = load_all_pecg_ta_models(models_base_path_4125, \"wav2vec\", class_names)\n",
    "\n",
    "#datasets_16 = get_pecg_ta_dataset(fs=16000)\n",
    "#class_names = next(iter(datasets_4125.values())).classes\n",
    "#\n",
    "##models_16 = load_all_pecg_ta_models(models_base_path_16, \"wav2vec\", class_names)\n",
    "#\n",
    "#datasets_pcg = get_pcg_ta_dataset(fs=4125)\n",
    "#class_names = next(iter(datasets_pcg.values())).classes\n",
    "#\n",
    "#models_pcg = load_all_pcg_ta_models(models_base_path_pcg, \"wav2vec\", class_names)\n",
    "\n",
    "#datasets_ecg = get_ecg_ta_dataset(fs=4125)\n",
    "#models_ecg = load_all_ecg_ta_models(models_base_path_pcg, \"wav2vec\", class_names)\n",
    "#\n",
    "#datasets_no_augment = get_pecg_ta_dataset(fs=16000, augment=False)\n",
    "#class_names = next(iter(datasets_4125.values())).classes\n",
    "#\n",
    "##models_no_augment = load_all_pecg_ta_models(models_base_path_pcg, \"wav2vec\", class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2860766b2b444bf6b26bea4985db1e5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                            | 0/61 [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected target size [64, 2], got [64]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#generate_roc_plots(models_pcg, datasets_pcg, \"roc/ta-pcg-4125\")\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#generate_roc_plots(models_ecg, datasets_ecg, \"roc/ta-ecg-4125\")\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mgenerate_roc_plots\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels_4125\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatasets_4125\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mroc/ta-pecg-4125\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#generate_roc_plots(models_16, datasets_16, \"roc/ta-pecg-16k\")\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#generate_roc_plots(models_no_augment, datasets_no_augment, \"roc/ta-pecg-16k-no-augment\")\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 20\u001b[0m, in \u001b[0;36mgenerate_roc_plots\u001b[0;34m(models, datasets, output_path)\u001b[0m\n\u001b[1;32m     17\u001b[0m fragment_tester \u001b[38;5;241m=\u001b[39m FineTunerFragmentTester(model, dataloader)\n\u001b[1;32m     18\u001b[0m patient_tester \u001b[38;5;241m=\u001b[39m FineTunerPatientTester(model, dataloader)\n\u001b[0;32m---> 20\u001b[0m fragment_tpr, fragment_fpr, _ \u001b[38;5;241m=\u001b[39m \u001b[43mfragment_tester\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroc_curve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m patient_tpr, patient_fpr, _ \u001b[38;5;241m=\u001b[39m patient_tester\u001b[38;5;241m.\u001b[39mroc_curve()\n\u001b[1;32m     23\u001b[0m fragment_fprs\u001b[38;5;241m.\u001b[39mappend(fragment_fpr)\n",
      "File \u001b[0;32m~/dev/heart_proj/src/python/classifier/testing.py:191\u001b[0m, in \u001b[0;36mFineTunerFragmentTester.roc_curve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    188\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_labels(labels)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    190\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(input_vals)\n\u001b[0;32m--> 191\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_vals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m    192\u001b[0m logits \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    193\u001b[0m logits[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m threshold\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/heart_proj-Md_C0Ba6/lib/python3.11/site-packages/torch/nn/functional.py:3086\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3084\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3085\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3086\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected target size [64, 2], got [64]"
     ]
    }
   ],
   "source": [
    "#generate_roc_plots(models_pcg, datasets_pcg, \"roc/ta-pcg-4125\")\n",
    "#generate_roc_plots(models_ecg, datasets_ecg, \"roc/ta-ecg-4125\")\n",
    "generate_roc_plots(models_4125, datasets_4125, \"roc/ta-pecg-4125\")\n",
    "#generate_roc_plots(models_16, datasets_16, \"roc/ta-pecg-16k\")\n",
    "#generate_roc_plots(models_no_augment, datasets_no_augment, \"roc/ta-pecg-16k-no-augment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16500\n"
     ]
    }
   ],
   "source": [
    "#model_base_path = '../../data/models/cinc-wav2vecwav2vec-4s-cinc-wav2vec-cnn-cinc-wav2vec-cnn-dec19.log'\n",
    "model_base_path = '../../data/models/cinc-wav2vecwav2vec-4s-cinc-wav2vec-cnn-cinc-wav2vec-cnn-dec19.log'\n",
    "\n",
    "datasets = get_pcg_cinc_dataset(fs=4125)\n",
    "class_names = next(iter(datasets.values())).classes\n",
    "models = load_all_pcg_cinc_cnn_models(model_base_path, \"wav2vec\", class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99523c4a598b41558eae5f0a394e3c78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                           | 0/120 [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "193cf5f0db04417888b6c84f4d4432c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                           | 0/120 [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fragment_fpr=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9915074309978769, 0.4002123142250531, 0.1746284501061571, 0.09394904458598727, 0.037154989384288746, 0.0031847133757961785, 0.0010615711252653928, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], fragment_tpr=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9922720247295209, 0.9397217928902627, 0.865533230293663, 0.49768160741885625, 0.03554868624420402, 0.00927357032457496, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7648418ab555457ab8c1b5d2632ecc97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                           | 0/120 [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "150e8dbd9c8d4e719747a9e893d4c1b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                           | 0/120 [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fragment_fpr=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.39384288747346075, 0.11942675159235669, 0.08333333333333333, 0.038747346072186835, 0.0037154989384288748, 0.002653927813163482, 0.0005307855626326964, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], fragment_tpr=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9845440494590417, 0.9119010819165378, 0.8253477588871716, 0.624420401854714, 0.1267387944358578, 0.015455950540958269, 0.010819165378670788, 0.0061823802163833074, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "563e01361599486b96586a5ae2ac22f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                           | 0/120 [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c578b5d0df2d423196b2259cbb3a6633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                           | 0/120 [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fragment_fpr=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.994692144373673, 0.351380042462845, 0.160828025477707, 0.09607218683651805, 0.03927813163481953, 0.004246284501061571, 0.0005307855626326964, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], fragment_tpr=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.990726429675425, 0.9489953632148377, 0.8717156105100463, 0.482225656877898, 0.0695517774343122, 0.00927357032457496, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63a438f1308c4b4e87ec854a2c57cf30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                           | 0/120 [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "135e1b6770a24cb1a1c2475fd86f8e80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                           | 0/120 [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fragment_fpr=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.37367303609341823, 0.17834394904458598, 0.09713375796178345, 0.034501061571125265, 0.0010615711252653928, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], fragment_tpr=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9953632148377125, 0.9675425038639877, 0.8979907264296755, 0.43431221020092736, 0.023183925811437404, 0.0, 0.0061823802163833074, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f779fe50c9da40eb87e86defe32c5954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                           | 0/120 [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbe30c7a15f04a2e967fc87b105fc210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                           | 0/120 [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fragment_fpr=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4166666666666667, 0.14012738853503184, 0.08545647558386411, 0.032908704883227176, 0.006369426751592357, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], fragment_tpr=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9922720247295209, 0.9428129829984544, 0.8438948995363215, 0.5718701700154559, 0.06491499227202473, 0.0, 0.0, 0.0, 0.0, 0.0015455950540958269, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c12f2b40bd86436d96216473646b2a9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                           | 0/120 [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbfb11565f7f4a9ba161fd359ee49221",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                           | 0/120 [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fragment_fpr=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9952229299363057, 0.3619957537154989, 0.1618895966029724, 0.09182590233545647, 0.029723991507430998, 0.0010615711252653928, 0.0, 0.0005307855626326964, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], fragment_tpr=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9953632148377125, 0.9659969088098919, 0.893353941267388, 0.47913446676970634, 0.0401854714064915, 0.0030911901081916537, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a27471a0a8144ddb961b8efade3e6a79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                           | 0/120 [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2403d6b7c4394729a7b1c72a0f6dc511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                           | 0/120 [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fragment_fpr=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.37154989384288745, 0.1751592356687898, 0.09925690021231423, 0.032908704883227176, 0.0031847133757961785, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], fragment_tpr=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9969088098918083, 0.9474497681607419, 0.8763523956723338, 0.4435857805255023, 0.023183925811437404, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a90bf1aae95438da0fc29e54e9e9c82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                           | 0/120 [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4889f6fdcff4479a13daeaf9a5f9035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                           | 0/120 [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fragment_fpr=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3880042462845011, 0.13853503184713375, 0.09288747346072186, 0.04617834394904458, 0.011146496815286623, 0.0005307855626326964, 0.0, 0.0005307855626326964, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], fragment_tpr=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.990726429675425, 0.9211746522411128, 0.8299845440494591, 0.5950540958268934, 0.1035548686244204, 0.010819165378670788, 0.0, 0.0015455950540958269, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dff12a9021b34f5aaa89d3dfe8a803a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                           | 0/120 [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7152e5e73524e68835c08a10f0744c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                           | 0/120 [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fragment_fpr=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9925690021231423, 0.46390658174097665, 0.18099787685774946, 0.09819532908704884, 0.025477707006369428, 0.0010615711252653928, 0.0005307855626326964, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], fragment_tpr=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9984544049459042, 0.9659969088098919, 0.874806800618238, 0.3848531684698609, 0.03863987635239567, 0.0030911901081916537, 0.0077279752704791345, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1759bd8a02394e39bcd74e3649850075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                           | 0/120 [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6179d3bd5e3a49f7aa30844e32d8e15b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                           | 0/120 [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fragment_fpr=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9920382165605095, 0.39118895966029726, 0.12154989384288747, 0.07908704883227176, 0.042993630573248405, 0.009023354564755838, 0.0010615711252653928, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], fragment_tpr=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9829984544049459, 0.919629057187017, 0.8562596599690881, 0.6367851622874807, 0.16692426584234932, 0.010819165378670788, 0.00463678516228748, 0.0015455950540958269, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "mean_fpr=array([5.30785563e-05, 7.26983670e-04, 1.59515282e-03, 2.22177186e-03,\n",
      "       2.84617844e-03, 3.54935219e-03, 4.25252595e-03, 4.93187599e-03,\n",
      "       5.61230386e-03, 6.29273173e-03, 6.97315960e-03, 7.62094657e-03,\n",
      "       8.25774084e-03, 8.92237027e-03, 9.62062987e-03, 1.03188895e-02,\n",
      "       1.10171491e-02, 1.17256156e-02, 1.24453862e-02, 1.31651568e-02,\n",
      "       1.38849273e-02, 1.46046979e-02, 1.53244685e-02, 1.60442391e-02,\n",
      "       1.67640097e-02, 1.74837803e-02, 1.82035508e-02, 1.89233214e-02,\n",
      "       1.96430920e-02, 2.03628626e-02, 2.10826332e-02, 2.18024038e-02,\n",
      "       2.25221744e-02, 2.32419449e-02, 2.39617155e-02, 2.46814861e-02,\n",
      "       2.54012567e-02, 2.61210273e-02, 2.68407979e-02, 2.76313446e-02,\n",
      "       2.84297960e-02, 2.92282474e-02, 3.00266987e-02, 3.08253179e-02,\n",
      "       3.16851473e-02, 3.26213258e-02, 3.35575043e-02, 3.44936829e-02,\n",
      "       3.54941976e-02, 3.65774002e-02, 3.77202072e-02, 3.88851132e-02,\n",
      "       4.00500192e-02, 4.12149253e-02, 4.23798313e-02, 4.35447373e-02,\n",
      "       4.47096433e-02, 4.59292927e-02, 4.72364436e-02, 4.85551438e-02,\n",
      "       4.99911286e-02, 5.14271134e-02, 5.28910100e-02, 5.44800362e-02,\n",
      "       5.61582639e-02, 5.78403762e-02, 5.95224885e-02, 6.12046008e-02,\n",
      "       6.28867131e-02, 6.45688254e-02, 6.62509377e-02, 6.79330501e-02,\n",
      "       6.96151624e-02, 7.12972747e-02, 7.29793870e-02, 7.46614993e-02,\n",
      "       7.63436116e-02, 7.80257239e-02, 7.97078362e-02, 8.13899485e-02,\n",
      "       8.30720608e-02, 8.47541731e-02, 8.64935506e-02, 8.86261932e-02,\n",
      "       9.09752018e-02, 9.36399616e-02, 9.69921048e-02, 1.02007067e-01,\n",
      "       1.08397366e-01, 1.15349921e-01, 1.23605990e-01, 1.34312213e-01,\n",
      "       1.51932765e-01, 1.70321946e-01, 1.95990439e-01, 2.29405144e-01,\n",
      "       2.69134448e-01, 3.22257519e-01, 4.15809792e-01, 1.00000000e+00]), fpr_2_5=array([0.00000000e+00, 2.56401149e-04, 5.56413779e-04, 8.14379662e-04,\n",
      "       1.10062533e-03, 1.77207571e-03, 2.44352609e-03, 3.08301110e-03,\n",
      "       3.37562587e-03, 3.59870714e-03, 3.82178841e-03, 4.04486968e-03,\n",
      "       4.26795095e-03, 4.70675464e-03, 5.40619226e-03, 6.08454499e-03,\n",
      "       6.75149368e-03, 7.44140792e-03, 8.15675631e-03, 8.87210471e-03,\n",
      "       9.58745310e-03, 1.03022556e-02, 1.10016932e-02, 1.17011308e-02,\n",
      "       1.24005684e-02, 1.31000060e-02, 1.37994436e-02, 1.44988813e-02,\n",
      "       1.51983189e-02, 1.58977565e-02, 1.65971941e-02, 1.72966317e-02,\n",
      "       1.79960693e-02, 1.86955070e-02, 1.93949446e-02, 2.00845184e-02,\n",
      "       2.07545299e-02, 2.14245414e-02, 2.20945529e-02, 2.27645644e-02,\n",
      "       2.34345759e-02, 2.41045874e-02, 2.47745989e-02, 2.54446104e-02,\n",
      "       2.61146219e-02, 2.67719176e-02, 2.73417083e-02, 2.79114991e-02,\n",
      "       2.84812898e-02, 2.90510806e-02, 2.96208713e-02, 3.01906621e-02,\n",
      "       3.07604528e-02, 3.13302436e-02, 3.19000343e-02, 3.24698251e-02,\n",
      "       3.30396158e-02, 3.40336679e-02, 3.57058564e-02, 3.69280688e-02,\n",
      "       3.79181313e-02, 3.89081938e-02, 3.98484015e-02, 4.17498205e-02,\n",
      "       4.38519428e-02, 4.59628055e-02, 4.79559254e-02, 4.97476369e-02,\n",
      "       5.15393484e-02, 5.33310599e-02, 5.51227714e-02, 5.69144829e-02,\n",
      "       5.87061944e-02, 6.04979059e-02, 6.22896174e-02, 6.40813288e-02,\n",
      "       6.58730403e-02, 6.76438139e-02, 6.93702339e-02, 7.10966539e-02,\n",
      "       7.28230738e-02, 7.45494938e-02, 7.61925420e-02, 7.78206719e-02,\n",
      "       7.94488018e-02, 8.19884845e-02, 8.75748337e-02, 9.06637575e-02,\n",
      "       9.22247388e-02, 9.74954485e-02, 1.07699466e-01, 1.17761336e-01,\n",
      "       1.27876223e-01, 1.36682665e-01, 1.48515534e-01, 1.58719551e-01,\n",
      "       1.88535159e-01, 2.57834992e-01, 3.27134825e-01, 1.00000000e+00]), fpr_97_5=array([4.11358811e-04, 1.32924726e-03, 2.75802076e-03, 3.52724375e-03,\n",
      "       4.30373164e-03, 5.10681626e-03, 6.16626532e-03, 7.22574210e-03,\n",
      "       8.25378777e-03, 9.31060322e-03, 1.03674187e-02, 1.11712671e-02,\n",
      "       1.18899222e-02, 1.26085772e-02, 1.33272323e-02, 1.40458873e-02,\n",
      "       1.47645423e-02, 1.54831974e-02, 1.62018524e-02, 1.69205074e-02,\n",
      "       1.76391625e-02, 1.83578175e-02, 1.90764726e-02, 1.98177304e-02,\n",
      "       2.05667890e-02, 2.13176853e-02, 2.20685816e-02, 2.28194779e-02,\n",
      "       2.35703742e-02, 2.43212704e-02, 2.50721667e-02, 2.58230630e-02,\n",
      "       2.65739593e-02, 2.73248556e-02, 2.80757519e-02, 2.88266481e-02,\n",
      "       2.95775444e-02, 3.03284407e-02, 3.10793370e-02, 3.18302333e-02,\n",
      "       3.26236132e-02, 3.34501451e-02, 3.42766770e-02, 3.51032090e-02,\n",
      "       3.60256212e-02, 3.71653280e-02, 3.84156873e-02, 3.97057078e-02,\n",
      "       4.11004476e-02, 4.24951874e-02, 4.38899271e-02, 4.52846669e-02,\n",
      "       4.66794067e-02, 4.80741465e-02, 4.94688862e-02, 5.08636260e-02,\n",
      "       5.22964777e-02, 5.37653259e-02, 5.52341740e-02, 5.67030221e-02,\n",
      "       5.81983156e-02, 5.97086044e-02, 6.12188932e-02, 6.27291819e-02,\n",
      "       6.42394707e-02, 6.57497595e-02, 6.72600482e-02, 6.87703370e-02,\n",
      "       7.02806257e-02, 7.17909145e-02, 7.33032114e-02, 7.48406911e-02,\n",
      "       7.63781708e-02, 7.79156505e-02, 7.94531302e-02, 8.09906099e-02,\n",
      "       8.25280896e-02, 8.40655693e-02, 8.56030490e-02, 8.71405287e-02,\n",
      "       8.87321242e-02, 9.04953534e-02, 9.24002170e-02, 9.62693384e-02,\n",
      "       1.00536423e-01, 1.04952187e-01, 1.09818581e-01, 1.15302617e-01,\n",
      "       1.21692836e-01, 1.29679702e-01, 1.39717962e-01, 1.51102296e-01,\n",
      "       1.81194911e-01, 2.19316230e-01, 2.58558894e-01, 2.97801559e-01,\n",
      "       3.37044223e-01, 3.77198069e-01, 6.28038716e-01, 1.00000000e+00])\n",
      "saving\n",
      "mean_fpr=array([0.00000000e+00, 4.17632892e-04, 8.97046871e-04, 1.57459540e-03,\n",
      "       2.26760405e-03, 2.98069450e-03, 3.68059038e-03, 4.39299141e-03,\n",
      "       5.10861795e-03, 5.82424449e-03, 6.57219463e-03, 7.33634407e-03,\n",
      "       8.04864112e-03, 8.76004136e-03, 9.47144160e-03, 1.01828418e-02,\n",
      "       1.08942421e-02, 1.16056423e-02, 1.23170426e-02, 1.30464753e-02,\n",
      "       1.37769706e-02, 1.45074659e-02, 1.52379611e-02, 1.59684564e-02,\n",
      "       1.66989517e-02, 1.74294469e-02, 1.81599422e-02, 1.88904375e-02,\n",
      "       1.96209327e-02, 2.03514280e-02, 2.10819233e-02, 2.18124185e-02,\n",
      "       2.25429138e-02, 2.32734091e-02, 2.40059427e-02, 2.48023777e-02,\n",
      "       2.55988128e-02, 2.63952479e-02, 2.71916830e-02, 2.79881180e-02,\n",
      "       2.87845531e-02, 2.95809882e-02, 3.03774233e-02, 3.12328623e-02,\n",
      "       3.21610067e-02, 3.31185539e-02, 3.40761010e-02, 3.50336482e-02,\n",
      "       3.59911953e-02, 3.69487424e-02, 3.79575661e-02, 3.90715132e-02,\n",
      "       4.01854603e-02, 4.12994074e-02, 4.24786913e-02, 4.37694439e-02,\n",
      "       4.51104442e-02, 4.64514445e-02, 4.77924447e-02, 4.91334450e-02,\n",
      "       5.04959810e-02, 5.19915497e-02, 5.36113669e-02, 5.53079125e-02,\n",
      "       5.70044580e-02, 5.87010036e-02, 6.03975492e-02, 6.20940948e-02,\n",
      "       6.37906403e-02, 6.54871859e-02, 6.71837315e-02, 6.88802771e-02,\n",
      "       7.05768226e-02, 7.22733682e-02, 7.39699138e-02, 7.56664594e-02,\n",
      "       7.73630049e-02, 7.90595505e-02, 8.07560961e-02, 8.24526417e-02,\n",
      "       8.41491872e-02, 8.58457328e-02, 8.75422784e-02, 8.95026497e-02,\n",
      "       9.20589377e-02, 9.52246749e-02, 9.92431060e-02, 1.04462426e-01,\n",
      "       1.10642717e-01, 1.17312008e-01, 1.25059794e-01, 1.35490435e-01,\n",
      "       1.52138556e-01, 1.70056629e-01, 1.96267050e-01, 2.30110220e-01,\n",
      "       2.70568503e-01, 3.29031290e-01, 4.37386217e-01, 1.00000000e+00]), fpr_2_5=array([0.00000000e+00, 5.80155769e-05, 3.19025537e-04, 8.13149902e-04,\n",
      "       1.30981488e-03, 1.85417675e-03, 2.21706612e-03, 2.57995548e-03,\n",
      "       2.94284484e-03, 3.30573421e-03, 3.91913149e-03, 4.65807335e-03,\n",
      "       5.39701520e-03, 6.13595706e-03, 6.82803879e-03, 7.43007265e-03,\n",
      "       8.03210651e-03, 8.63414036e-03, 9.23617422e-03, 9.95511810e-03,\n",
      "       1.06531652e-02, 1.13512123e-02, 1.20492594e-02, 1.27473065e-02,\n",
      "       1.34453536e-02, 1.41434008e-02, 1.47517102e-02, 1.53545970e-02,\n",
      "       1.59574839e-02, 1.65603707e-02, 1.71632575e-02, 1.77661443e-02,\n",
      "       1.83690311e-02, 1.89719179e-02, 1.95748047e-02, 2.01776916e-02,\n",
      "       2.07805784e-02, 2.13834652e-02, 2.19863520e-02, 2.25892388e-02,\n",
      "       2.31921256e-02, 2.37950125e-02, 2.43978993e-02, 2.50007861e-02,\n",
      "       2.56036729e-02, 2.62065597e-02, 2.68094465e-02, 2.74123334e-02,\n",
      "       2.80152202e-02, 2.86181070e-02, 2.92209938e-02, 2.98238806e-02,\n",
      "       3.04267674e-02, 3.10296542e-02, 3.21389014e-02, 3.38793409e-02,\n",
      "       3.56197804e-02, 3.73602198e-02, 3.86469921e-02, 3.96752964e-02,\n",
      "       4.06742868e-02, 4.23597657e-02, 4.41253803e-02, 4.59556903e-02,\n",
      "       4.77860002e-02, 4.96163101e-02, 5.14466201e-02, 5.32769300e-02,\n",
      "       5.51072399e-02, 5.69375499e-02, 5.87678598e-02, 6.05981697e-02,\n",
      "       6.24284797e-02, 6.42587896e-02, 6.60890995e-02, 6.79194095e-02,\n",
      "       6.97497194e-02, 7.15800293e-02, 7.34103393e-02, 7.52406492e-02,\n",
      "       7.70709591e-02, 7.89012691e-02, 8.07315790e-02, 8.24442179e-02,\n",
      "       8.60192188e-02, 8.98812502e-02, 9.15139121e-02, 9.48246743e-02,\n",
      "       9.76194637e-02, 1.02537807e-01, 1.11722819e-01, 1.21003558e-01,\n",
      "       1.30284297e-01, 1.38915320e-01, 1.48845775e-01, 1.58126513e-01,\n",
      "       1.83590985e-01, 2.69207065e-01, 3.40363042e-01, 1.00000000e+00]), fpr_97_5=array([0.00000000e+00, 7.78054659e-04, 1.43363802e-03, 2.36740509e-03,\n",
      "       3.43077933e-03, 4.49756433e-03, 5.59735827e-03, 6.67234703e-03,\n",
      "       7.74733580e-03, 8.82232456e-03, 9.89731332e-03, 1.09723021e-02,\n",
      "       1.16454349e-02, 1.23116173e-02, 1.29777998e-02, 1.36439822e-02,\n",
      "       1.43101647e-02, 1.49763472e-02, 1.56425296e-02, 1.63087121e-02,\n",
      "       1.69748945e-02, 1.76410770e-02, 1.83072594e-02, 1.89734419e-02,\n",
      "       1.96396243e-02, 2.03058068e-02, 2.09719892e-02, 2.16381717e-02,\n",
      "       2.23043542e-02, 2.29705366e-02, 2.37002692e-02, 2.44987700e-02,\n",
      "       2.52972708e-02, 2.60957716e-02, 2.68942724e-02, 2.79740522e-02,\n",
      "       2.92947881e-02, 3.06155239e-02, 3.19362597e-02, 3.32569956e-02,\n",
      "       3.45777314e-02, 3.58984673e-02, 3.72192031e-02, 3.86726980e-02,\n",
      "       4.01375531e-02, 4.16024082e-02, 4.30672633e-02, 4.45321184e-02,\n",
      "       4.59969735e-02, 4.74618287e-02, 4.89266838e-02, 5.03915389e-02,\n",
      "       5.18563940e-02, 5.33212491e-02, 5.47861042e-02, 5.62509594e-02,\n",
      "       5.77158145e-02, 5.91806696e-02, 6.06455247e-02, 6.21103798e-02,\n",
      "       6.35752349e-02, 6.50400900e-02, 6.65049452e-02, 6.79698003e-02,\n",
      "       6.94346554e-02, 7.08995105e-02, 7.23674183e-02, 7.38868512e-02,\n",
      "       7.54062840e-02, 7.69257168e-02, 7.84451497e-02, 7.99645825e-02,\n",
      "       8.14840153e-02, 8.30034482e-02, 8.45228810e-02, 8.60423139e-02,\n",
      "       8.75617467e-02, 8.90811795e-02, 9.06006124e-02, 9.21200452e-02,\n",
      "       9.36394780e-02, 9.51589109e-02, 9.66783437e-02, 9.81977765e-02,\n",
      "       9.97172094e-02, 1.01613397e-01, 1.08579255e-01, 1.15545112e-01,\n",
      "       1.22510970e-01, 1.30216778e-01, 1.38016422e-01, 1.47102763e-01,\n",
      "       1.82902717e-01, 2.22825885e-01, 2.63749430e-01, 3.04672975e-01,\n",
      "       3.45998540e-01, 3.91688751e-01, 6.55503455e-01, 1.00000000e+00])\n",
      "saving\n"
     ]
    }
   ],
   "source": [
    "generate_roc_plots(models, datasets, \"roc/cinc-pcg-cnn-4125\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create PaCMAP Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Optional\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pacmap\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_dimension_reduction(data: np.ndarray, labels: np.ndarray, transform, name: str, output_path: Optional[str] = None, normalise: bool = True, fit_embeddings: bool = True, original_data = None):\n",
    "    scaler = MinMaxScaler((-1, 1))\n",
    "\n",
    "    if fit_embeddings: \n",
    "        X_transformed = transform.fit_transform(data) # type: ignore\n",
    "        if normalise:\n",
    "            X_transformed = scaler.fit_transform(X_transformed)\n",
    "    else:\n",
    "        if original_data is None:\n",
    "            raise ValueError(\"Must provide the original data with the fit_transform.\")\n",
    "        # If it's PaCMAP, we need the 'basis' argument. If it's PCA, no basis is required.\n",
    "        if isinstance(transform, pacmap.PaCMAP):\n",
    "            if original_data is None:\n",
    "                raise ValueError(\"Must provide the original data with the PaCMAP transform.\")\n",
    "            X_transformed = transform.transform(data, basis=original_data)  # PaCMAP requires the basis argument\n",
    "        else:\n",
    "            X_transformed = transform.transform(data)\n",
    "\n",
    "    plt.figure()\n",
    "    normal_indices = np.where(labels == 0)[0]\n",
    "    plt.scatter(\n",
    "        X_transformed[normal_indices, 0],\n",
    "        X_transformed[normal_indices, 1],\n",
    "        color='blue', \n",
    "        label='Normal', \n",
    "        s=0.8,\n",
    "    )\n",
    "    # Plot \"Abnormal\" points\n",
    "    abnormal_indices = np.where(labels == 1)[0]\n",
    "    plt.scatter(\n",
    "        X_transformed[abnormal_indices, 0],\n",
    "        X_transformed[abnormal_indices, 1],\n",
    "        color='red', \n",
    "        label='Abnormal', \n",
    "        s=0.8,\n",
    "    ) # type: ignore\n",
    "    plt.legend()\n",
    "    plt.title(f\"{name} Visualisation of Features\")\n",
    "    plt.xlabel(\"Component 1\")\n",
    "    plt.ylabel(\"Component 2\")\n",
    "\n",
    "    if output_path is None:\n",
    "        plt.show()    \n",
    "    else:\n",
    "        plt.savefig(output_path)\n",
    "\n",
    "    return transform, data\n",
    "\n",
    "def plot_pacmap_features(data: np.ndarray, labels: np.ndarray, output_path: Optional[str] = None, normalise: bool = True, embedding_transform = None, original_data = None):\n",
    "    embedding = pacmap.PaCMAP(n_components=2) if embedding_transform is None else embedding_transform  # type: ignore\n",
    "    fit_embeddings = embedding_transform is None  # Only fit if embedding_transform is None\n",
    "    fit_embedding, original_data = plot_dimension_reduction(data, labels, embedding, \"PaCMAP\", output_path=output_path, normalise=normalise, fit_embeddings=fit_embeddings, original_data=original_data)\n",
    "\n",
    "    return fit_embedding, original_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classifier.dataloaders import create_dataloaders\n",
    "from classifier.testing import FineTunerFragmentTester, FineTunerPatientTester\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')  # or 'QtAgg' or 'MacOSX' depending on your system\n",
    "plt.ion\n",
    "\n",
    "dataloader = create_dataloaders(datasets, aux_type)\n",
    "features, labels = FineTunerFragmentTester(model.model_ft, dataloader).embeddings()\n",
    "\n",
    "plot_pacmap_features(features, labels, output_path=\"pacmap/pcg-cinc-4125\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "heart_proj-Md_C0Ba6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
